{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "513747ff",
   "metadata": {},
   "source": [
    "# Classifying subreddits posts from UEFA Champions League and English Premier League"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ca899d",
   "metadata": {},
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb6a6a7",
   "metadata": {},
   "source": [
    "EuroFootball is a monthly football magazine covering both the major leagues in Europe (Premier League, Serie A,La Liga etc) as well as the European competition such as the UEFA Champions League and Europa League.\n",
    "\n",
    "One of the columns in EuroFootball publishes readers' comments on any topics on the teams they support. Invariably, these comments usually revolves around players' performance, celebration of winning an important match and criticism of the manager's selection.\n",
    "\n",
    "Fans submit their comments through email, the best ones of which are then reviewed and selected by the editor for publication. However, this is an extremely time-consuming process where the submitted comments needs to be sorted into topics before the editor review. In particular, comments regarding the English Premier League and the UEFA Champions League forms the majority.\n",
    "\n",
    "The editorial team in EuroFootball has therefore tasked its data analytics team with creating a model which can predict whether comments submitted by readers has to do with the English Premier League or the UEFA Champions League.\n",
    "\n",
    "In order for the model to predict and categorise the comments correctly, the model needs to be trained on labeled data. The data analytics team therefore propose to scrape posts from Reddit under the the r/championsleague and r/PremierLeague for the labeled data.\n",
    "\n",
    "**About Reddit**\n",
    "\n",
    "Reddit is an American social news aggregation, web content rating, and discussion website. Registered members submit content to the site such as links, text posts, images, and videos, which are then voted up or down by other members. Posts are organized by subject into user-created boards called \"communities\" or \"subreddits\", which cover a variety of topics such as news, politics, religion, science, movies, video games, music, books, sports, fitness, cooking, pets, and image-sharing. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96cb61a",
   "metadata": {},
   "source": [
    "## Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cae83a",
   "metadata": {},
   "source": [
    "The main objective of the project is as follows:\n",
    "\n",
    "1. Create a comments classifier model which can categorize readers comments into topics regarding UEFA Champions League and English Premier League\n",
    "2. Identify the most important words that distinguish comments regarding UEFA Champions League and English Premier League"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3720be47",
   "metadata": {},
   "source": [
    "## Date Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34beccb5",
   "metadata": {},
   "source": [
    "We will first scrape the posts from reddit using [*Pushshift's*](https://github.com/pushshift/api) API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cf4f2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk import pos_tag\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "pd.options.display.max_colwidth = 400"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969c58a4",
   "metadata": {},
   "source": [
    "### Custom Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c126b0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reddit_post(subreddit, post):\n",
    "    '''This function takes in the title of the subreddit in the form of a string and the number of posts \n",
    "    without empty and removed selftext and returns a DataFrame containing the required information'''\n",
    "    \n",
    "    url = 'https://api.pushshift.io/reddit/search/submission'\n",
    "    params = {\n",
    "    'subreddit': subreddit,\n",
    "    'size': 100    \n",
    "    }\n",
    "    res = requests.get(url, params) #scrape the first 100 posts\n",
    "    print(res.status_code)\n",
    "    df = pd.DataFrame(res.json()['data'])\n",
    "    earl_date = df.loc[99, 'created_utc'] #capture the earliest date of the first 100 posts for reference \n",
    "                                          #for the loop later\n",
    "    \n",
    "    # number of posts after removing those with empty string or '[removed]' in the selftext column\n",
    "    num_posts = df.loc[(df['selftext']!='')&(df['selftext']!='[removed]'), :].shape[0]\n",
    "    \n",
    "    # loop to retrieve the required numbe of non-empty or non-removed posts until 'post' is reached\n",
    "    while num_posts < post:\n",
    "        params = {\n",
    "        'subreddit': subreddit,\n",
    "        'size': 100,\n",
    "        'before': earl_date\n",
    "        }\n",
    "        res = requests.get(url, params)\n",
    "        print(res.status_code)\n",
    "        if res.status_code != 200:\n",
    "            continue\n",
    "        df2 = pd.DataFrame(res.json()['data'])\n",
    "        earl_date = df2.tail(1)['created_utc']\n",
    "        df = pd.concat([df, df2]) # concatenate the newly scraped dataframe to the current one\n",
    "        num_posts = df.loc[(df['selftext']!='')&(df['selftext']!='[removed]'), :].shape[0]\n",
    "        print(f'{num_posts} posts without empty or removed selftext scraped')\n",
    "    \n",
    "    return df.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1878d6",
   "metadata": {},
   "source": [
    "### Scrape the subreddits "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3045f6a",
   "metadata": {},
   "source": [
    "We will first scrape for the r/PremierLeague and r/champiosnleague subreddit for 1000 posts with text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da05cc9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "200\n",
      "95 posts without empty or removed selftext scraped\n",
      "200\n",
      "145 posts without empty or removed selftext scraped\n",
      "200\n",
      "198 posts without empty or removed selftext scraped\n",
      "200\n",
      "242 posts without empty or removed selftext scraped\n",
      "200\n",
      "285 posts without empty or removed selftext scraped\n",
      "200\n",
      "318 posts without empty or removed selftext scraped\n",
      "200\n",
      "358 posts without empty or removed selftext scraped\n",
      "200\n",
      "402 posts without empty or removed selftext scraped\n",
      "200\n",
      "444 posts without empty or removed selftext scraped\n",
      "200\n",
      "495 posts without empty or removed selftext scraped\n",
      "200\n",
      "534 posts without empty or removed selftext scraped\n",
      "200\n",
      "581 posts without empty or removed selftext scraped\n",
      "200\n",
      "627 posts without empty or removed selftext scraped\n",
      "200\n",
      "662 posts without empty or removed selftext scraped\n",
      "200\n",
      "698 posts without empty or removed selftext scraped\n",
      "200\n",
      "744 posts without empty or removed selftext scraped\n",
      "200\n",
      "795 posts without empty or removed selftext scraped\n",
      "200\n",
      "841 posts without empty or removed selftext scraped\n",
      "200\n",
      "888 posts without empty or removed selftext scraped\n",
      "200\n",
      "941 posts without empty or removed selftext scraped\n",
      "200\n",
      "986 posts without empty or removed selftext scraped\n",
      "200\n",
      "1026 posts without empty or removed selftext scraped\n"
     ]
    }
   ],
   "source": [
    "df_epl = get_reddit_post('PremierLeague', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0cd26a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "200\n",
      "61 posts without empty or removed selftext scraped\n",
      "200\n",
      "78 posts without empty or removed selftext scraped\n",
      "200\n",
      "99 posts without empty or removed selftext scraped\n",
      "200\n",
      "123 posts without empty or removed selftext scraped\n",
      "200\n",
      "150 posts without empty or removed selftext scraped\n",
      "200\n",
      "178 posts without empty or removed selftext scraped\n",
      "200\n",
      "204 posts without empty or removed selftext scraped\n",
      "200\n",
      "233 posts without empty or removed selftext scraped\n",
      "200\n",
      "270 posts without empty or removed selftext scraped\n",
      "200\n",
      "308 posts without empty or removed selftext scraped\n",
      "200\n",
      "333 posts without empty or removed selftext scraped\n",
      "200\n",
      "362 posts without empty or removed selftext scraped\n",
      "200\n",
      "384 posts without empty or removed selftext scraped\n",
      "200\n",
      "404 posts without empty or removed selftext scraped\n",
      "200\n",
      "434 posts without empty or removed selftext scraped\n",
      "200\n",
      "460 posts without empty or removed selftext scraped\n",
      "200\n",
      "480 posts without empty or removed selftext scraped\n",
      "200\n",
      "505 posts without empty or removed selftext scraped\n",
      "200\n",
      "540 posts without empty or removed selftext scraped\n",
      "200\n",
      "590 posts without empty or removed selftext scraped\n",
      "200\n",
      "622 posts without empty or removed selftext scraped\n",
      "200\n",
      "643 posts without empty or removed selftext scraped\n",
      "200\n",
      "663 posts without empty or removed selftext scraped\n",
      "200\n",
      "706 posts without empty or removed selftext scraped\n",
      "200\n",
      "728 posts without empty or removed selftext scraped\n",
      "200\n",
      "742 posts without empty or removed selftext scraped\n",
      "200\n",
      "757 posts without empty or removed selftext scraped\n",
      "200\n",
      "795 posts without empty or removed selftext scraped\n",
      "200\n",
      "820 posts without empty or removed selftext scraped\n",
      "200\n",
      "845 posts without empty or removed selftext scraped\n",
      "200\n",
      "860 posts without empty or removed selftext scraped\n",
      "200\n",
      "864 posts without empty or removed selftext scraped\n",
      "200\n",
      "874 posts without empty or removed selftext scraped\n",
      "200\n",
      "888 posts without empty or removed selftext scraped\n",
      "200\n",
      "908 posts without empty or removed selftext scraped\n",
      "200\n",
      "926 posts without empty or removed selftext scraped\n",
      "200\n",
      "946 posts without empty or removed selftext scraped\n",
      "200\n",
      "967 posts without empty or removed selftext scraped\n",
      "502\n",
      "200\n",
      "979 posts without empty or removed selftext scraped\n",
      "200\n",
      "986 posts without empty or removed selftext scraped\n",
      "200\n",
      "997 posts without empty or removed selftext scraped\n",
      "200\n",
      "1009 posts without empty or removed selftext scraped\n"
     ]
    }
   ],
   "source": [
    "df_cl = get_reddit_post('championsleague', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2369820",
   "metadata": {},
   "source": [
    "### Inspect the DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3effda7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>all_awardings</th>\n",
       "      <th>allow_live_comments</th>\n",
       "      <th>author</th>\n",
       "      <th>author_flair_background_color</th>\n",
       "      <th>author_flair_css_class</th>\n",
       "      <th>author_flair_richtext</th>\n",
       "      <th>author_flair_template_id</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>author_flair_text_color</th>\n",
       "      <th>...</th>\n",
       "      <th>crosspost_parent</th>\n",
       "      <th>crosspost_parent_list</th>\n",
       "      <th>gallery_data</th>\n",
       "      <th>is_gallery</th>\n",
       "      <th>media_metadata</th>\n",
       "      <th>discussion_type</th>\n",
       "      <th>distinguished</th>\n",
       "      <th>author_cakeday</th>\n",
       "      <th>poll_data</th>\n",
       "      <th>edited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>mined_it</td>\n",
       "      <td>transparent</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'a': ':liv:', 'e': 'emoji', 'u': 'https://emoji.redditmedia.com/7b4b1cctklg51_t5_2scup/liv'}]</td>\n",
       "      <td>aabec794-dcbc-11ea-8fe1-0e19d6f66ce5</td>\n",
       "      <td>:liv:</td>\n",
       "      <td>dark</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>Tesus4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>Tesus4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>AutoModerator</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>Sports_Hat</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 86 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index all_awardings  allow_live_comments         author  \\\n",
       "0      0            []                False       mined_it   \n",
       "1      1            []                False         Tesus4   \n",
       "2      2            []                False         Tesus4   \n",
       "3      3            []                False  AutoModerator   \n",
       "4      4            []                False     Sports_Hat   \n",
       "\n",
       "  author_flair_background_color author_flair_css_class  \\\n",
       "0                   transparent                   None   \n",
       "1                           NaN                   None   \n",
       "2                           NaN                   None   \n",
       "3                           NaN                   None   \n",
       "4                           NaN                   None   \n",
       "\n",
       "                                                                             author_flair_richtext  \\\n",
       "0  [{'a': ':liv:', 'e': 'emoji', 'u': 'https://emoji.redditmedia.com/7b4b1cctklg51_t5_2scup/liv'}]   \n",
       "1                                                                                               []   \n",
       "2                                                                                               []   \n",
       "3                                                                                               []   \n",
       "4                                                                                               []   \n",
       "\n",
       "               author_flair_template_id author_flair_text  \\\n",
       "0  aabec794-dcbc-11ea-8fe1-0e19d6f66ce5             :liv:   \n",
       "1                                   NaN              None   \n",
       "2                                   NaN              None   \n",
       "3                                   NaN              None   \n",
       "4                                   NaN              None   \n",
       "\n",
       "  author_flair_text_color  ... crosspost_parent crosspost_parent_list  \\\n",
       "0                    dark  ...              NaN                   NaN   \n",
       "1                     NaN  ...              NaN                   NaN   \n",
       "2                     NaN  ...              NaN                   NaN   \n",
       "3                     NaN  ...              NaN                   NaN   \n",
       "4                     NaN  ...              NaN                   NaN   \n",
       "\n",
       "   gallery_data is_gallery media_metadata discussion_type  distinguished  \\\n",
       "0           NaN        NaN            NaN             NaN            NaN   \n",
       "1           NaN        NaN            NaN             NaN            NaN   \n",
       "2           NaN        NaN            NaN             NaN            NaN   \n",
       "3           NaN        NaN            NaN             NaN            NaN   \n",
       "4           NaN        NaN            NaN             NaN            NaN   \n",
       "\n",
       "   author_cakeday  poll_data edited  \n",
       "0             NaN        NaN    NaN  \n",
       "1             NaN        NaN    NaN  \n",
       "2             NaN        NaN    NaN  \n",
       "3             NaN        NaN    NaN  \n",
       "4             NaN        NaN    NaN  \n",
       "\n",
       "[5 rows x 86 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_epl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77e666cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>all_awardings</th>\n",
       "      <th>allow_live_comments</th>\n",
       "      <th>author</th>\n",
       "      <th>author_flair_css_class</th>\n",
       "      <th>author_flair_richtext</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>author_flair_type</th>\n",
       "      <th>author_fullname</th>\n",
       "      <th>author_is_blocked</th>\n",
       "      <th>...</th>\n",
       "      <th>og_title</th>\n",
       "      <th>gilded</th>\n",
       "      <th>rte_mode</th>\n",
       "      <th>author_id</th>\n",
       "      <th>brand_safe</th>\n",
       "      <th>suggested_sort</th>\n",
       "      <th>approved_at_utc</th>\n",
       "      <th>banned_at_utc</th>\n",
       "      <th>view_count</th>\n",
       "      <th>author_created_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>zaviews</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_106sw9</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>GoalooES</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_b6693nfb</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>Structure-Diligent</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_7fv9g819</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>GoalooES</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_b6693nfb</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>MatchCaster</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_7h7t33at</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 96 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index all_awardings allow_live_comments              author  \\\n",
       "0      0            []               False             zaviews   \n",
       "1      1            []               False            GoalooES   \n",
       "2      2            []               False  Structure-Diligent   \n",
       "3      3            []               False            GoalooES   \n",
       "4      4            []               False         MatchCaster   \n",
       "\n",
       "  author_flair_css_class author_flair_richtext author_flair_text  \\\n",
       "0                   None                    []              None   \n",
       "1                   None                    []              None   \n",
       "2                   None                    []              None   \n",
       "3                   None                    []              None   \n",
       "4                   None                    []              None   \n",
       "\n",
       "  author_flair_type author_fullname author_is_blocked  ... og_title gilded  \\\n",
       "0              text       t2_106sw9             False  ...      NaN    NaN   \n",
       "1              text     t2_b6693nfb             False  ...      NaN    NaN   \n",
       "2              text     t2_7fv9g819             False  ...      NaN    NaN   \n",
       "3              text     t2_b6693nfb             False  ...      NaN    NaN   \n",
       "4              text     t2_7h7t33at             False  ...      NaN    NaN   \n",
       "\n",
       "  rte_mode author_id brand_safe  suggested_sort approved_at_utc banned_at_utc  \\\n",
       "0      NaN       NaN        NaN             NaN             NaN           NaN   \n",
       "1      NaN       NaN        NaN             NaN             NaN           NaN   \n",
       "2      NaN       NaN        NaN             NaN             NaN           NaN   \n",
       "3      NaN       NaN        NaN             NaN             NaN           NaN   \n",
       "4      NaN       NaN        NaN             NaN             NaN           NaN   \n",
       "\n",
       "  view_count author_created_utc  \n",
       "0        NaN                NaN  \n",
       "1        NaN                NaN  \n",
       "2        NaN                NaN  \n",
       "3        NaN                NaN  \n",
       "4        NaN                NaN  \n",
       "\n",
       "[5 rows x 96 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cl.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f1062a",
   "metadata": {},
   "source": [
    "### Save Scraped Data to csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb3425f",
   "metadata": {},
   "source": [
    "Both the 'championsleague' and 'PremierLeague' subreddits are saved and processed in the next notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a51a5d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_epl.to_csv('../data/epl.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16e2cd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cl.to_csv('../data/cl.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
